ğŸ¤– ZNET â€” Local AI Chat App (Streamlit + Ollama)
ZNET is a lightweight and fast AI chat interface built using Streamlit and Ollama.  
It allows you to run LLMs locally on your system with real-time streaming responses.

---

## âœ¨ Features
- âš¡ Local LLM chat using Ollama  
- ğŸ¨ Clean Streamlit UI  
- ğŸ”„ Real-time message streaming  
- ğŸ”§ Easy model switching  
- ğŸ’¬ Chat history stored in session  

---

## ğŸ“¦ Requirements

Make sure you have:

- Python 3.9+
- Streamlit
- Ollama installed locally  
  ğŸ‘‰ Install Ollama: https://ollama.com/download

---

## ğŸ”§ Installation

 Install Python dependencies

pip install streamlit ollama

 Pull the required models (optional)

ollama pull qwen2.5:0.5b
ollama pull qwen2.5-coder:1.5b


---

ğŸš€ Run the App

streamlit run app.py

App will start at:

http://localhost:8501


---

ğŸ§  Supported Models

ZNET includes two small, fast models:

qwen2.5:0.5b

qwen2.5-coder:1.5b


You can add more models by editing the models list in the code.

âš ï¸ Deployment Note

ZNET cannot be deployed on Vercel or Streamlit Cloud, because:

Ollama requires a local machine or a custom server

Cloud-free hosting platforms do not support Ollama runtime


To deploy online, use:

AWS EC2

Google Cloud VM

DigitalOcean Droplet

Any VPS with Ollama installed



---

ğŸ¤ Contributing

Pull requests are welcome.
Feel free to open issues for improvements or bugs.


---

ğŸ“„ License

MIT License
